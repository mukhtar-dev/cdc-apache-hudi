# ğŸ§ª Change Data Capture with Apache Hudi, Debezium, Kafka & Spark

This project demonstrates a complete Change Data Capture (CDC) pipeline using PostgreSQL, Debezium, Kafka, Apache Hudi, and Apache Spark. It captures real-time changes from a PostgreSQL database, streams them via Kafka, writes to a Hudi data lake, and incrementally updates an aggregated table.

---

## ğŸ”§ Technologies Used

- **PostgreSQL** â€“ Source database with logical replication enabled.
- **Debezium** â€“ Captures database changes as events and sends them to Kafka.
- **Kafka & Zookeeper** â€“ Message streaming platform.
- **Schema Registry** â€“ Manages Avro schemas used in CDC events.
- **Apache Hudi** â€“ Incrementally stores CDC data into a data lake.
- **Apache Spark** â€“ Reads CDC data, transforms, and updates Hudi tables.

---

## ğŸ—ï¸ Architecture Overview
```text

PostgreSQL (retail_transactions table)
â”‚
â–¼
Debezium (CDC)
â”‚
â–¼
Kafka (sampleTopic)
â”‚
â–¼
Spark Streaming (kafka_to_hudi_stream.py)
â”‚
â–¼
Hudi Base Table (/hudi_data/retails_table)
â”‚
â–¼
Batch Spark Job (update_hudi_agg_table.py)
â”‚
â–¼
Hudi Aggregated Table (/hudi_data/aggregated_orders)


---

## ğŸ“ Project Structure
```text
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ connector.json
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init.sh               # PostgreSQL initialization
â”œâ”€â”€ jars_dir/                 # Optional Spark JARs
â””â”€â”€ spark-apps/
    â”œâ”€â”€ kafka_to_hudi_stream.py
    â”œâ”€â”€ update_hudi_agg_table.py
    â”œâ”€â”€ query_hudi_base_table.py
    â””â”€â”€ query_hudi_agg_table.py


---

## ğŸš€ How to Run

### 1. Start Docker Environment

```bash
cd C:\DataProjects\cdc-apache-hudi
docker-compose up --build


### 2. Prepare PostgreSQL

```bash
docker exec -it hudidb bash
psql -U postgres -f /docker-entrypoint-initdb.d/init.sh
psql -U postgres -d dev -c "SELECT * FROM V1.retail_transactions;"


### 3. Register Debezium Connector

```bash
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" \
  http://localhost:8083/connectors/ -d @connector.json

### 4. Create Hudi Data Lake Directory
```bash

### 4. Create Hudi Data Lake Directory
```bash
### 4. Create Hudi Data Lake Directory
```bash
### 4. Create Hudi Data Lake Directory
```bash
### 4. Create Hudi Data Lake Directory
```bash
